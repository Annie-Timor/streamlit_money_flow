# pages/4_ETF_Extremum_Proximity.py
import streamlit as st
import akshare as ak
import pandas as pd
import numpy as np
from scipy.signal import find_peaks
from datetime import datetime, timedelta
import pandas_ta as ta # å¯¼å…¥åº“ï¼Œé€šå¸¸ç®€å†™ä¸º ta

# --- åˆå§‹åŒ– session_state ---
if 'debug_logs' not in st.session_state:
    st.session_state.debug_logs = []
if 'max_debug_logs' not in st.session_state:
    st.session_state.max_debug_logs = 100

# --- è°ƒè¯•ä¿¡æ¯è®°å½•å‡½æ•° ---
def add_debug_log(message):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_msg = f"[{timestamp}] {message}"
    st.session_state.debug_logs.append(log_msg)
    if len(st.session_state.debug_logs) > st.session_state.max_debug_logs:
        st.session_state.debug_logs.pop(0)
    # print(f"DEBUG: {message}")

# --- å¯¼å…¥ETFæ˜ å°„ ---
try:
    from etf_industry_map import ETF_INDUSTRY_MAPPINGS
except ImportError:
    st.sidebar.warning("`etf_industry_map.py` ä¸­æœªæ‰¾åˆ° `ETF_INDUSTRY_MAPPINGS`ã€‚å°†ä½¿ç”¨é¢„è®¾çš„è¡Œä¸šETFåˆ—è¡¨ã€‚")
    ETF_INDUSTRY_MAPPINGS = {
        "åŠå¯¼ä½“": "512480", "åŒ»ç–—å™¨æ¢°": "159883", "é…¿é…’è¡Œä¸š": "512690",
        "é“¶è¡Œ": "512800", "è¯åˆ¸": "512880", "å…‰ä¼è®¾å¤‡": "159863",
    }

try:
    from etf_industry_map import ETF_SELECT_MAPPINGS
except ImportError:
    st.sidebar.warning("`etf_industry_map.py` ä¸­æœªæ‰¾åˆ° `ETF_SELECT_MAPPINGS`ã€‚å°†ä½¿ç”¨é¢„è®¾çš„è‡ªé€‰ETFåˆ—è¡¨ã€‚")
    ETF_SELECT_MAPPINGS = {
        "çº³æŒ‡ETF": "513100", "æ²ªæ·±300": "510300", "é»„é‡‘ETF": "518880",
    }


st.set_page_config(page_title="ETFæå€¼ç‚¹é è¿‘åˆ†æ (ATR)", layout="wide")
st.title("ğŸ” æ‰¹é‡ETFæå€¼ç‚¹é è¿‘åˆ†æ (åŸºäºATR)")
st.markdown("åˆ†æå¤šä¸ªETFå½“å‰ä»·æ ¼æ˜¯å¦æ¥è¿‘å…¶å†å²å±€éƒ¨é«˜ç‚¹æˆ–ä½ç‚¹ï¼ˆä½¿ç”¨ATRåˆ¤æ–­é è¿‘ç¨‹åº¦ï¼‰ã€‚")

# --- ä¾§è¾¹æ å‚æ•°é…ç½® ---
st.sidebar.header("åˆ†æå‚æ•°é…ç½®")

# 1. NEW: é€‰æ‹©ETFæ¥æº
st.sidebar.subheader("æ•°æ®æºé€‰æ‹©")
etf_source = st.sidebar.radio(
    "é€‰æ‹©ETFåˆ—è¡¨:",
    ("è¡Œä¸šETF", "è‡ªé€‰ETF"),
    key="etf_source_choice",
    help="é€‰æ‹©è¦åˆ†æçš„ETFåˆ—è¡¨æ¥æºã€‚"
)

# 2. å†å²æ•°æ®è·å–å¹´é™
st.sidebar.subheader("æ•°æ®å‘¨æœŸ")
history_years_options = [1, 2]
selected_history_years = st.sidebar.selectbox(
    "å†å²æ•°æ®å¹´é™:", options=history_years_options, index=1,
    help="è·å–å¤šå°‘å¹´çš„å†å²æ•°æ®è¿›è¡Œæå€¼ç‚¹è®¡ç®—ã€‚"
)

# 3. æå€¼ç‚¹è®¡ç®—å‚æ•°
st.sidebar.subheader("æå€¼ç‚¹è¯†åˆ«å‚æ•°")
peak_distance_default = 10
peak_distance_input_batch = st.sidebar.number_input(
    "æœ€å°å³°é—´è· (å¤©/æ•°æ®ç‚¹):", min_value=5, max_value=60, value=peak_distance_default, step=1,
    key="peak_dist_batch", help="è¯†åˆ«çš„æ³¢å³°/æ³¢è°·ä¹‹é—´è‡³å°‘ç›¸éš”å¤šå°‘ä¸ªäº¤æ˜“æ—¥ã€‚"
)
peak_prominence_std_factor = st.sidebar.slider(
    "çªèµ·é«˜åº¦å› å­ (ä¹˜ä»¥æ ‡å‡†å·®):", min_value=0.1, max_value=2.0, value=0.5, step=0.1,
    key="peak_prom_factor_batch", help="æ³¢å³°/æ³¢è°·çš„çªèµ·ç¨‹åº¦ï¼Œä»¥æ”¶ç›˜ä»·æ ‡å‡†å·®çš„å€æ•°è¡¡é‡ã€‚"
)

# 4. é è¿‘åˆ†æå‚æ•° (åŸºäºATR)
st.sidebar.subheader("é è¿‘ç¨‹åº¦åˆ†æå‚æ•° (ATR)")
atr_period_proximity = st.sidebar.slider(
    "ATRå‘¨æœŸ (ç”¨äºé è¿‘åˆ¤æ–­):", min_value=5, max_value=50, value=14, step=1,
    key="atr_period_prox_batch", help="è®¡ç®—ATRæ—¶ä½¿ç”¨çš„å‘¨æœŸå¤©æ•°ã€‚"
)
atr_multiplier_options_prox = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
atr_multiplier_proximity = st.sidebar.selectbox(
    "é è¿‘ATRå€æ•° (n):", options=atr_multiplier_options_prox, index=2,
    key="atr_multiplier_prox_batch", help="å½“å‰ä»·æ ¼ä¸æå€¼ç‚¹ç›¸å·® n * ATR ä»¥å†…è¢«è®¤ä¸ºæ˜¯â€œé è¿‘â€ã€‚"
)

# 5. é€‰æ‹©åˆ†æçš„æå€¼ç±»å‹
st.sidebar.subheader("åˆ†æç±»å‹")
analyze_maxima = st.sidebar.checkbox("åˆ†æé è¿‘å±€éƒ¨é«˜ç‚¹", value=True, key="analyze_max_batch")
analyze_minima = st.sidebar.checkbox("åˆ†æé è¿‘å±€éƒ¨ä½ç‚¹", value=True, key="analyze_min_batch")

# 6. NEW: é€‰æ‹©ç»“æœå±•ç¤ºæ–¹å¼
st.sidebar.subheader("ç»“æœå±•ç¤ºæ–¹å¼")
display_mode = st.sidebar.radio(
    "é€‰æ‹©ç»“æœå±•ç¤ºæ–¹å¼:",
    ("è”åˆæ˜¾ç¤º", "åˆ†å¼€æ˜¾ç¤º"),
    index=0,  # é»˜è®¤é€‰æ‹©â€œè”åˆæ˜¾ç¤ºâ€
    key="display_mode_choice"
)

# 7. åˆ†ææŒ‰é’®
analyze_button = st.sidebar.button("ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ", key="analyze_extremes_btn")


# --- æ•°æ®è·å–ä¸å¤„ç†å‡½æ•° ---
@st.cache_data(ttl=86400)
def fetch_raw_etf_data_with_atr(etf_code, years_of_history, atr_period_for_calc):
    """è·å–ETFåŸå§‹OHLCæ•°æ®å¹¶è®¡ç®—ATRã€‚"""
    add_debug_log(f"Fetching raw data & ATR for {etf_code}, {years_of_history} years, ATR({atr_period_for_calc})")
    end_date = datetime.now()
    start_date = end_date - timedelta(days=int(years_of_history * 365.25))
    start_str = start_date.strftime('%Y%m%d')
    end_str = end_date.strftime('%Y%m%d')
    try:
        df = ak.fund_etf_hist_em(symbol=etf_code, period="daily", start_date=start_str, end_date=end_str, adjust="qfq")
        if df.empty or not all(col in df.columns for col in ['æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½']):
            return pd.DataFrame(), f"æ•°æ®ä¸è¶³æˆ–ç¼ºå°‘å¿…è¦åˆ—(æ”¶ç›˜/æœ€é«˜/æœ€ä½) for {etf_code}"

        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ']).dt.normalize()
        df.set_index('æ—¥æœŸ', inplace=True)
        rename_map = {'å¼€ç›˜': 'Open', 'æœ€é«˜': 'High', 'æœ€ä½': 'Low', 'æ”¶ç›˜': 'Close', 'æˆäº¤é‡': 'Volume'}
        df.rename(columns=rename_map, inplace=True)
        
        cols_to_numeric = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in cols_to_numeric:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        df.dropna(subset=['High', 'Low', 'Close'], inplace=True)

        if len(df) <= atr_period_for_calc:
            df['ATR'] = np.nan
            add_debug_log(f"Data points ({len(df)}) insufficient for ATR({atr_period_for_calc}) for {etf_code}. ATR set to NaN.")
            return df, f"æ•°æ®ç‚¹ä¸è¶³ä»¥è®¡ç®—ATR for {etf_code}" if df.empty else None
        
        # df['ATR'] = talib.ATR(df['High'].astype(float), df['Low'].astype(float), df['Close'].astype(float), timeperiod=atr_period_for_calc)
        # å¦‚æœæ‚¨æƒ³è‡ªå®šä¹‰æ–°åˆ—çš„åç§°ï¼Œå¯ä»¥è¿™æ ·åšï¼š
        df['ATR'] = df.ta.atr(high='High', low='Low', close='Close', length=atr_period_for_calc)
        
        return df, None
    except Exception as e:
        return pd.DataFrame(), f"è·å–æˆ–è®¡ç®—ATR for {etf_code} å‡ºé”™: {e}"

@st.cache_data(ttl=7200)
def find_extremes_from_series(etf_code, close_series, p_dist, p_prom_factor):
    """ä»å·²è·å–çš„æ”¶ç›˜ä»·åºåˆ—ä¸­è®¡ç®—æå€¼ç‚¹ã€‚"""
    add_debug_log(f"Finding extremes for {etf_code} with p_dist={p_dist}, p_prom_factor={p_prom_factor}")
    if close_series.empty:
        return None, None, f"æ— æ”¶ç›˜ä»·åºåˆ— for {etf_code}"

    if len(close_series) < p_dist * 2:
        return None, None, f"æ•°æ®ç‚¹ä¸è¶³ ({len(close_series)}) for {etf_code} to find peaks with distance {p_dist}"

    price_std = close_series.std()
    actual_prominence = price_std * p_prom_factor if price_std > 0.00001 else 0.01

    try:
        max_locs, _ = find_peaks(close_series, distance=p_dist, prominence=actual_prominence)
        max_points_data = [(close_series.index[loc], close_series.iloc[loc]) for loc in max_locs]

        min_locs, _ = find_peaks(-close_series, distance=p_dist, prominence=actual_prominence)
        min_points_data = [(close_series.index[loc], close_series.iloc[loc]) for loc in min_locs]
        
        return max_points_data, min_points_data, None
    except Exception as e:
        return None, None, f"find_peaks for {etf_code} å‡ºé”™: {e}"

# --- ä¸»é€»è¾‘ ---
if analyze_button:
    # 1. æ ¹æ®é€‰æ‹©ç¡®å®šè¦åˆ†æçš„ETFåˆ—è¡¨
    if etf_source == "è¡Œä¸šETF":
        selected_etf_map = ETF_INDUSTRY_MAPPINGS
    else: # è‡ªé€‰ETF
        selected_etf_map = ETF_SELECT_MAPPINGS
    
    if not selected_etf_map:
        st.error(f"é€‰æ‹©çš„ â€œ{etf_source}â€ åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚è¯·æ£€æŸ¥ `etf_industry_map.py`ã€‚")
    elif not analyze_maxima and not analyze_minima:
        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ç§åˆ†æç±»å‹ï¼ˆé è¿‘å±€éƒ¨é«˜ç‚¹æˆ–é è¿‘å±€éƒ¨ä½ç‚¹ï¼‰ã€‚")
    else:
        etf_codes_to_analyze = list(selected_etf_map.values())
        total_etfs = len(etf_codes_to_analyze)
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨åˆ—è¡¨
        results_near_maxima = []
        results_near_minima = []
        all_results = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()

        for i, etf_code_iter in enumerate(etf_codes_to_analyze):
            etf_name = [k for k, v in selected_etf_map.items() if v == etf_code_iter][0] or "N/A"
            status_text.info(f"æ­£åœ¨åˆ†æ: {etf_name} ({etf_code_iter}) - [{i+1}/{total_etfs}]")

            df_etf_full, fetch_error = fetch_raw_etf_data_with_atr(
                etf_code_iter, selected_history_years, atr_period_proximity
            )
            if fetch_error or df_etf_full.empty:
                st.caption(f"è·³è¿‡ {etf_code_iter}: {fetch_error or 'æ— æœ‰æ•ˆæ•°æ®'}")
                progress_bar.progress((i + 1) / total_etfs)
                continue
            
            close_prices_series = df_etf_full['Close']
            
            maxima, minima, extremes_error = find_extremes_from_series(
                etf_code_iter, close_prices_series, peak_distance_input_batch, peak_prominence_std_factor
            )
            if extremes_error:
                st.caption(f"è·³è¿‡ {etf_code_iter} (æå€¼ç‚¹è¯†åˆ«): {extremes_error}")
                progress_bar.progress((i + 1) / total_etfs)
                continue

            if df_etf_full.empty or 'Close' not in df_etf_full.columns or 'ATR' not in df_etf_full.columns:
                st.caption(f"è·³è¿‡ {etf_code_iter}: æ•°æ®ä¸å®Œæ•´æ— æ³•è¿›è¡Œé è¿‘åˆ†æã€‚")
                progress_bar.progress((i + 1) / total_etfs)
                continue

            current_price = df_etf_full['Close'].iloc[-1]
            current_atr = df_etf_full['ATR'].iloc[-1]

            if pd.isna(current_price) or pd.isna(current_atr) or current_atr <= 0:
                st.caption(f"è·³è¿‡ {etf_code_iter}: å½“å‰ä»·æ ¼æˆ–ATRæ— æ•ˆ (Price: {current_price}, ATR: {current_atr})ã€‚")
                progress_bar.progress((i + 1) / total_etfs)
                continue

            # åˆ†æé è¿‘å±€éƒ¨é«˜ç‚¹
            if analyze_maxima and maxima:
                for peak_date, peak_price in maxima:
                    upper_bound = peak_price + (atr_multiplier_proximity * current_atr)
                    lower_bound = peak_price - (atr_multiplier_proximity * current_atr)
                    if lower_bound <= current_price <= upper_bound:
                        proximity_in_atr = (current_price - peak_price) / current_atr if current_atr else np.nan
                        # è®¡ç®—è·ç¦»æå¤§å€¼çš„ç™¾åˆ†æ¯”
                        current_percent = (peak_price - current_price) / current_price * 100
                        result_item = {
                            "ETFä»£ç ": etf_code_iter,
                            "åç§°": etf_name,
                            "å½“å‰ä»·æ ¼": current_price,
                            "æå€¼ç‚¹æ—¥æœŸ": peak_date.strftime('%Y-%m-%d'),
                            "æå€¼ç‚¹ä»·æ ¼": peak_price,
                            "å½“å‰ATR": current_atr,
                            "è·ç¦»ATRå€æ•°": proximity_in_atr,
                            "è·ç¦»ç™¾åˆ†æ¯”": current_percent
                        }
                        if display_mode == "è”åˆæ˜¾ç¤º":
                            result_item["åˆ†æç±»å‹"] = "é è¿‘é«˜ç‚¹"
                            all_results.append(result_item)
                        else: # åˆ†å¼€æ˜¾ç¤º
                            results_near_maxima.append(result_item)
            
            # åˆ†æé è¿‘å±€éƒ¨ä½ç‚¹
            if analyze_minima and minima:
                for valley_date, valley_price in minima:
                    upper_bound = valley_price + (atr_multiplier_proximity * current_atr)
                    lower_bound = valley_price - (atr_multiplier_proximity * current_atr)
                    if lower_bound <= current_price <= upper_bound:
                        proximity_in_atr = (current_price - valley_price) / current_atr if current_atr else np.nan
                        # è®¡ç®—è·ç¦»æå°å€¼çš„ç™¾åˆ†æ¯”
                        current_percent = (valley_price - current_price) / current_price * 100
                        result_item = {
                            "ETFä»£ç ": etf_code_iter,
                            "åç§°": etf_name,
                            "å½“å‰ä»·æ ¼": current_price,
                            "æå€¼ç‚¹æ—¥æœŸ": valley_date.strftime('%Y-%m-%d'),
                            "æå€¼ç‚¹ä»·æ ¼": valley_price,
                            "å½“å‰ATR": current_atr,
                            "è·ç¦»ATRå€æ•°": proximity_in_atr,
                            "è·ç¦»ç™¾åˆ†æ¯”": current_percent
                        }
                        if display_mode == "è”åˆæ˜¾ç¤º":
                            result_item["åˆ†æç±»å‹"] = "é è¿‘ä½ç‚¹"
                            all_results.append(result_item)
                        else: # åˆ†å¼€æ˜¾ç¤º
                            results_near_minima.append(result_item)

            progress_bar.progress((i + 1) / total_etfs)
        
        status_text.success(f"æ‰¹é‡åˆ†æå®Œæˆï¼å…±åˆ†æ {total_etfs} ä¸ªETFã€‚")

        # --- æ˜¾ç¤ºç»“æœ ---
        st.markdown("---")

        # 1. NEW: åˆ›å»ºå¹¶æ˜¾ç¤ºè§¦åŠæå€¼ç‚¹çš„ETFåç§°æ‘˜è¦
        found_etf_names = set()
        # ä»æ‰€æœ‰ç»“æœä¸­æ”¶é›†ETFåç§°
        if display_mode == "è”åˆæ˜¾ç¤º":
            for item in all_results:
                found_etf_names.add(f"{item['åç§°']} ({item['ETFä»£ç ']})")
        else: # åˆ†å¼€æ˜¾ç¤ºæ¨¡å¼
            for item in results_near_maxima:
                found_etf_names.add(f"{item['åç§°']} ({item['ETFä»£ç ']})")
            for item in results_near_minima:
                found_etf_names.add(f"{item['åç§°']} ({item['ETFä»£ç ']})")

        st.subheader("ğŸ“£ è§¦åŠæå€¼ç‚¹ETFä¸€è§ˆ")
        if found_etf_names:
            # æ’åºåæ˜¾ç¤ºï¼Œæ›´æ¸…æ™°
            sorted_names = sorted(list(found_etf_names))
            # ä½¿ç”¨å¤šåˆ—å¸ƒå±€ä»¥è·å¾—æ›´å¥½çš„è§†è§‰æ•ˆæœ
            num_columns = 5
            cols = st.columns(num_columns)
            for i, name in enumerate(sorted_names):
                with cols[i % num_columns]:
                    st.success(name)
        else:
            st.info("æœ¬æ¬¡åˆ†ææœªå‘ç°ä»»ä½•è§¦åŠæå€¼ç‚¹çš„ETFã€‚")
        
        st.markdown("---") # æ·»åŠ åˆ†éš”çº¿ï¼Œå°†æ‘˜è¦ä¸è¯¦æƒ…åˆ†å¼€

        if display_mode == "è”åˆæ˜¾ç¤º":
            st.subheader(f"ğŸ“Š ç»¼åˆåˆ†æç»“æœ (èŒƒå›´: æå€¼ç‚¹ Â± {atr_multiplier_proximity} * ATR)")
            if all_results:
                df_all = pd.DataFrame(all_results)
                # æ ¼å¼åŒ–è¾“å‡º
                df_all['å½“å‰ä»·æ ¼'] = df_all['å½“å‰ä»·æ ¼'].map('{:.3f}'.format)
                df_all['æå€¼ç‚¹ä»·æ ¼'] = df_all['æå€¼ç‚¹ä»·æ ¼'].map('{:.3f}'.format)
                df_all['å½“å‰ATR'] = df_all['å½“å‰ATR'].map('{:.4f}'.format)
                df_all['è·ç¦»ATRå€æ•°'] = df_all['è·ç¦»ATRå€æ•°'].map('{:.2f}'.format)
                df_all['è·ç¦»ç™¾åˆ†æ¯”'] = df_all['è·ç¦»ç™¾åˆ†æ¯”'].map('{:.2f}'.format)
                # è°ƒæ•´åˆ—é¡ºåº
                cols_order = ["ETFä»£ç ", "åç§°", "åˆ†æç±»å‹", "å½“å‰ä»·æ ¼", "æå€¼ç‚¹æ—¥æœŸ", "æå€¼ç‚¹ä»·æ ¼", "å½“å‰ATR", "è·ç¦»ATRå€æ•°", "è·ç¦»ç™¾åˆ†æ¯”"]
                st.dataframe(df_all[cols_order].reset_index(drop=True), use_container_width=True)
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶ï¼ˆé è¿‘å±€éƒ¨é«˜ç‚¹æˆ–ä½ç‚¹ï¼‰çš„ETFã€‚")

        else: # åˆ†å¼€æ˜¾ç¤º
            if analyze_maxima:
                st.subheader(f"ğŸ“ˆ é è¿‘å†å²å±€éƒ¨é«˜ç‚¹ (èŒƒå›´: Â± {atr_multiplier_proximity} * ATR) çš„ETF")
                if results_near_maxima:
                    df_max = pd.DataFrame(results_near_maxima)
                    df_max['å½“å‰ä»·æ ¼'] = df_max['å½“å‰ä»·æ ¼'].map('{:.3f}'.format)
                    df_max['æå€¼ç‚¹ä»·æ ¼'] = df_max['æå€¼ç‚¹ä»·æ ¼'].map('{:.3f}'.format)
                    df_max['å½“å‰ATR'] = df_max['å½“å‰ATR'].map('{:.4f}'.format)
                    df_max['è·ç¦»ATRå€æ•°'] = df_max['è·ç¦»ATRå€æ•°'].map('{:.2f}'.format)
                    df_max['è·ç¦»ç™¾åˆ†æ¯”'] = df_max['è·ç¦»ç™¾åˆ†æ¯”'].map('{:.2f}'.format)
                    st.dataframe(df_max.reset_index(drop=True), use_container_width=True)
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶ï¼ˆé è¿‘å±€éƒ¨é«˜ç‚¹ï¼‰çš„ETFã€‚")
                st.markdown("---")

            if analyze_minima:
                st.subheader(f"ğŸ“‰ é è¿‘å†å²å±€éƒ¨ä½ç‚¹ (èŒƒå›´: Â± {atr_multiplier_proximity} * ATR) çš„ETF")
                if results_near_minima:
                    df_min = pd.DataFrame(results_near_minima)
                    df_min['å½“å‰ä»·æ ¼'] = df_min['å½“å‰ä»·æ ¼'].map('{:.3f}'.format)
                    df_min['æå€¼ç‚¹ä»·æ ¼'] = df_min['æå€¼ç‚¹ä»·æ ¼'].map('{:.3f}'.format)
                    df_min['å½“å‰ATR'] = df_min['å½“å‰ATR'].map('{:.4f}'.format)
                    df_min['è·ç¦»ATRå€æ•°'] = df_min['è·ç¦»ATRå€æ•°'].map('{:.2f}'.format)
                    df_min['è·ç¦»ç™¾åˆ†æ¯”'] = df_min['è·ç¦»ç™¾åˆ†æ¯”'].map('{:.2f}'.format)
                    st.dataframe(df_min.reset_index(drop=True), use_container_width=True)
                else:
                    st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶ï¼ˆé è¿‘å±€éƒ¨ä½ç‚¹ï¼‰çš„ETFã€‚")
        
        # --- æ˜¾ç¤ºå¯æŠ˜å çš„è°ƒè¯•æ—¥å¿— ---
        with st.expander("æ˜¾ç¤º/éšè— è¯¦ç»†è°ƒè¯•æ—¥å¿—", expanded=False):
            if st.session_state.debug_logs:
                for log_entry in reversed(st.session_state.debug_logs):
                    st.code(log_entry, language=None)
                if st.button("æ¸…é™¤è°ƒè¯•æ—¥å¿—", key="clear_debug_logs_btn"):
                    st.session_state.debug_logs = []
                    st.rerun()
            else:
                st.info("æš‚æ— è°ƒè¯•æ—¥å¿—ã€‚")
else:
    st.info("è¯·åœ¨å·¦ä¾§é…ç½®å‚æ•°ï¼Œç„¶åç‚¹å‡»â€œå¼€å§‹æ‰¹é‡åˆ†æâ€æŒ‰é’®ã€‚")